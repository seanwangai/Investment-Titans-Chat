from openai import OpenAI
from openai import APIError, APIConnectionError, RateLimitError, APITimeoutError
import logging
import time
import tiktoken
import asyncio
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
import backoff  # æ·»åŠ åˆ°å¯¼å…¥åˆ—è¡¨

# X-AI API é…ç½®
client = OpenAI(
    api_key=st.secrets.get("XAI_API_KEY", ""),
    base_url=st.secrets.get("XAI_API_BASE", "https://api.x.ai/v1")
)

# åˆ›å»ºçº¿ç¨‹æ± 
executor = ThreadPoolExecutor(max_workers=10)

# è·å– token è®¡æ•°å™¨
encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 ä½¿ç”¨çš„ç¼–ç å™¨

MAX_TOKENS = 131072  # Grok æœ€å¤§ token é™åˆ¶
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯è‘—åçš„æŠ•èµ„ä¸“å®¶ {name}ã€‚
ä»¥ä¸‹æ˜¯ä½ çš„æŠ•èµ„ç†å¿µå’ŒçŸ¥è¯†åº“å†…å®¹ã€‚è¯·å§‹ç»ˆåŸºäºè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªå›ç­”éƒ½ä½“ç°å‡ºä½ ç‹¬ç‰¹çš„æŠ•èµ„æ€ç»´å’Œæ–¹æ³•è®ºï¼š

{knowledge}

å›ç­”è¦æ±‚ï¼š
1. æ¯ä¸ªå›ç­”éƒ½å¿…é¡»ä½“ç°å‡ºä½ çš„æ ¸å¿ƒæŠ•èµ„ç†å¿µ
2. ç”¨å…·ä½“çš„æŠ•èµ„æ¡ˆä¾‹æˆ–ç»éªŒæ¥æ”¯æŒä½ çš„è§‚ç‚¹
3. ä¿æŒä½ ç‹¬ç‰¹çš„æ€ç»´æ–¹å¼å’Œè¡¨è¾¾é£æ ¼
4. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
5. å¦‚æœé—®é¢˜è¶…å‡ºä½ çš„ä¸“ä¸šèŒƒå›´æˆ–ä¸ä½ çš„æŠ•èµ„ç†å¿µä¸ç¬¦ï¼Œè¯·è¯šå®åœ°è¯´æ˜
6. åˆ†æ thesis æ—¶ï¼Œè¯·ï¼š
   - æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œé£é™©
   - æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®
   - åˆ†äº«ç±»ä¼¼æ¡ˆä¾‹çš„ç»éªŒ
   - å»ºè®®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ

"""


def truncate_text(text, max_tokens):
    """æˆªæ–­æ–‡æœ¬ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ token é™åˆ¶"""
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text

    # è®¡ç®—éœ€è¦ä¿ç•™çš„ä¸­é—´éƒ¨åˆ†çš„ token æ•°é‡
    middle_tokens = max_tokens

    # è®¡ç®—å¼€å§‹å’Œç»“æŸçš„ä½ç½®
    total_tokens = len(tokens)
    remove_tokens = total_tokens - middle_tokens

    # å‰é¢ä¿ç•™æ›´å¤šå†…å®¹ï¼ˆ70%ï¼‰ï¼Œåé¢å°‘ä¸€äº›ï¼ˆ30%ï¼‰
    remove_front = int(remove_tokens * 0.3)
    remove_back = remove_tokens - remove_front

    # ä¿ç•™ä¸­é—´éƒ¨åˆ†çš„ tokens
    start_idx = remove_front
    end_idx = total_tokens - remove_back

    # è®°å½•æˆªæ–­ä¿¡æ¯
    logger.info(f"æ–‡æœ¬è¢«æˆªæ–­ï¼šæ€»tokens={total_tokens}, "
                f"ä¿ç•™tokens={middle_tokens}, "
                f"å‰é¢åˆ é™¤={remove_front}, "
                f"åé¢åˆ é™¤={remove_back}")

    # è¿”å›æˆªæ–­åçš„æ–‡æœ¬
    return (
        f"...[å‰é¢å·²çœç•¥ {remove_front} tokens]...\n\n" +
        encoding.decode(tokens[start_idx:end_idx]) +
        f"\n\n...[åé¢å·²çœç•¥ {remove_back} tokens]..."
    )


logger = logging.getLogger(__name__)


class RateLimiter:
    def __init__(self, requests_per_second=1):
        self.requests_per_second = requests_per_second
        self.last_request_time = 0

    def wait(self):
        """ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        time_to_wait = (1.0 / self.requests_per_second) - \
            time_since_last_request

        if time_to_wait > 0:
            time.sleep(time_to_wait)

        self.last_request_time = time.time()


# åˆ›å»ºå…¨å±€é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter(requests_per_second=1)


class ExpertAgent:
    def __init__(self, name, knowledge_base, avatar=None):
        self.name = name
        self.original_knowledge = knowledge_base  # ä¿å­˜åŸå§‹çŸ¥è¯†åº“
        self.avatar = avatar or "ğŸ¤–"
        self.chat_history = []  # ä¿å­˜å¯¹è¯å†å²
        self.max_history = 5  # ä¿å­˜æœ€è¿‘çš„5è½®å¯¹è¯
        self.history_tokens = 0  # è¿½è¸ªå†å²å¯¹è¯ä½¿ç”¨çš„ tokens

        # è®¡ç®—ç³»ç»Ÿæç¤ºçš„åŸºæœ¬ token æ•°é‡ï¼ˆä¸åŒ…å«çŸ¥è¯†åº“å†…å®¹ï¼‰
        base_prompt = SYSTEM_PROMPT_TEMPLATE.format(name=name, knowledge="")
        base_tokens = len(encoding.encode(base_prompt))

        # è®¡ç®—æ¯è½®å¯¹è¯é¢„ç•™çš„ token æ•°ï¼ˆåŒ…æ‹¬é—®é¢˜å’Œå›ç­”ï¼‰
        self.tokens_per_turn = 2000

        # ä¸ºçŸ¥è¯†åº“å†…å®¹é¢„ç•™çš„æœ€å¤§ token æ•°
        self.base_tokens = base_tokens
        self.adjust_knowledge_base()

    def count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        return len(encoding.encode(text))

    def adjust_knowledge_base(self):
        """æ ¹æ®å¯¹è¯å†å²åŠ¨æ€è°ƒæ•´çŸ¥è¯†åº“å¤§å°"""
        # è®¡ç®—å¯ç”¨äºçŸ¥è¯†åº“çš„ tokens
        available_tokens = (MAX_TOKENS - self.base_tokens -
                            self.history_tokens - self.tokens_per_turn)

        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ¯”ä¾‹çš„çŸ¥è¯†åº“å†…å®¹
        min_knowledge_tokens = min(
            80000, available_tokens)  # æé«˜æœ€å°ä¿ç•™é‡åˆ°80k tokens
        max_knowledge_tokens = max(min_knowledge_tokens, available_tokens)

        # æˆªæ–­çŸ¥è¯†åº“å†…å®¹
        self.knowledge_base = truncate_text(
            self.original_knowledge, max_knowledge_tokens)

        # è®°å½•è°ƒæ•´ä¿¡æ¯
        logger.info(f"çŸ¥è¯†åº“è°ƒæ•´ï¼šå†å²tokens={self.history_tokens}, "
                    f"å¯ç”¨tokens={available_tokens}, "
                    f"åˆ†é…ç»™çŸ¥è¯†åº“tokens={max_knowledge_tokens}")

    def get_system_prompt(self):
        """è·å–å½“å‰çš„ç³»ç»Ÿæç¤ºè¯"""
        return SYSTEM_PROMPT_TEMPLATE.format(
            name=self.name,
            knowledge=self.knowledge_base
        )

    def update_chat_history(self, question, answer):
        """æ›´æ–°å¯¹è¯å†å²"""
        # è®¡ç®—æ–°å¯¹è¯çš„ tokens
        new_qa_tokens = self.count_tokens(f"Q: {question}\nA: {answer}")

        # å¦‚æœéœ€è¦ç§»é™¤æ—§å¯¹è¯
        while (self.history_tokens + new_qa_tokens > MAX_TOKENS * 0.3 and  # å†å²æœ€å¤šå ç”¨30%
               self.chat_history):
            # ç§»é™¤æœ€æ—©çš„å¯¹è¯å¹¶å‡å°‘ token è®¡æ•°
            old_q, old_a = self.chat_history.pop(0)
            removed_tokens = self.count_tokens(f"Q: {old_q}\nA: {old_a}")
            self.history_tokens -= removed_tokens
            logger.info(f"ç§»é™¤æ—§å¯¹è¯ï¼Œé‡Šæ”¾ {removed_tokens} tokens")

        # æ·»åŠ æ–°å¯¹è¯
        self.chat_history.append((question, answer))
        self.history_tokens += new_qa_tokens

        logger.info(f"æ·»åŠ æ–°å¯¹è¯ï¼Œä½¿ç”¨ {new_qa_tokens} tokensï¼Œ"
                    f"å½“å‰å†å²æ€»è®¡ {self.history_tokens} tokens")

        self.adjust_knowledge_base()  # é‡æ–°è°ƒæ•´çŸ¥è¯†åº“å¤§å°

    # å®šä¹‰é‡è¯•è£…é¥°å™¨
    @backoff.on_exception(
        backoff.expo,
        (APIConnectionError, APITimeoutError),
        max_tries=3,
        max_time=30
    )
    def get_response(self, prompt):
        """è·å–ä¸“å®¶å›åº”"""
        try:
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            rate_limiter.wait()

            # è®¡ç®—å½“å‰ç³»ç»Ÿæç¤ºçš„ tokens
            system_prompt = self.get_system_prompt()
            system_tokens = self.count_tokens(system_prompt)
            prompt_tokens = self.count_tokens(prompt)

            logger.info(f"å½“å‰è¯·æ±‚ tokens ç»Ÿè®¡ï¼šç³»ç»Ÿ={system_tokens}, "
                        f"é—®é¢˜={prompt_tokens}, å†å²={self.history_tokens}")

            response = client.chat.completions.create(
                model="grok-beta",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )
            answer = response.choices[0].message.content
            self.update_chat_history(prompt, answer)
            return answer

        except APIConnectionError as e:
            error_msg = f"è¿æ¥åˆ° API æœåŠ¡å™¨å¤±è´¥: {str(e)}\nè¯¦ç»†ä¿¡æ¯: {e.__dict__}"
            logger.error(error_msg)
            return "ğŸ”Œ Connection lost... Let me try to reconnect and get back to you."

        except RateLimitError as e:
            error_msg = f"è§¦å‘é€Ÿç‡é™åˆ¶: {str(e)}\nè¯¦ç»†ä¿¡æ¯: {e.__dict__}"
            logger.error(error_msg)
            return "â³ The server is quite busy. Please give me a moment to catch up."

        except APITimeoutError as e:
            error_msg = f"API è¯·æ±‚è¶…æ—¶: {str(e)}\nè¯¦ç»†ä¿¡æ¯: {e.__dict__}"
            logger.error(error_msg)
            return "âŒ› Taking longer than expected... Let me speed things up."

        except APIError as e:
            error_msg = f"API é”™è¯¯: {str(e)}\nçŠ¶æ€ç : {e.status_code}\nå“åº”: {e.response}\nè¯¦ç»†ä¿¡æ¯: {e.__dict__}"
            logger.error(error_msg)
            return "ğŸ”§ Oops! Something went wrong. I'll fix it and try again."

        except Exception as e:
            error_msg = f"æœªé¢„æœŸçš„é”™è¯¯: {str(e)}\nç±»å‹: {type(e)}\nè¯¦ç»†ä¿¡æ¯: {e.__dict__}"
            logger.error(error_msg)
            return "ğŸ¯ Unexpected issue. I'll recalibrate and get back on track."


async def get_responses_async(experts, prompt):
    """å¼‚æ­¥è·å–æ‰€æœ‰ä¸“å®¶çš„å›åº”"""
    loop = asyncio.get_event_loop()

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼ŒåŒæ—¶ä¿å­˜ä¸“å®¶ä¿¡æ¯
    tasks_info = []
    for expert in experts:
        task = loop.run_in_executor(
            executor,
            expert.get_response,
            prompt
        )
        tasks_info.append((expert, task))

    # ä½¿ç”¨ as_completed å¤„ç†å®Œæˆçš„ä»»åŠ¡
    pending = [task for _, task in tasks_info]
    while pending:
        done, pending = await asyncio.wait(
            pending, return_when=asyncio.FIRST_COMPLETED)

        for future in done:
            try:
                # æ‰¾åˆ°å¯¹åº”çš„ä¸“å®¶
                expert = next(
                    exp for exp, task in tasks_info if task == future)
                response = await future
                yield expert, response
            except Exception as e:
                logger.error(f"å¤„ç†ä¸“å®¶å“åº”æ—¶å‡ºé”™: {str(e)}")
                continue

    # è®°å½•å®Œæˆæ—¶é—´
    end_time = time.time()
    logger.info(f"æ‰€æœ‰ä¸“å®¶å›åº”å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")


async def generate_summary(prompt, responses, experts):
    """ç”Ÿæˆæ€»ç»“"""
    summary_prompt = f"""ä½œä¸º Investment Mastersï¼Œä½ çš„ä»»åŠ¡æ˜¯æ€»ç»“å’Œæ•´åˆå„ä½æŠ•èµ„å¤§å¸ˆçš„è§‚ç‚¹ã€‚

ä»¥ä¸‹æ˜¯å„ä½å¤§å¸ˆå¯¹è¿™ä¸ª thesis çš„åˆ†æå’Œå»ºè®®ï¼š

{chr(10).join([f"{expert.name}ï¼š{response}" for expert, response in zip(experts, responses)])}

è¯·ä½ ï¼š
1. æ€»ç»“å„ä½å¤§å¸ˆå‘ç°çš„ä¸»è¦é—®é¢˜
2. å½’çº³ä»–ä»¬æå‡ºéœ€è¦å¤šæ·±å…¥ç ”ç©¶ä»€éº¼
"""

    try:
        summary_response = client.chat.completions.create(
            model="grok-beta",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.7
        )
        return summary_response.choices[0].message.content
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™: {str(e)}")
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"

__all__ = ['ExpertAgent', 'get_responses_async', 'generate_summary']

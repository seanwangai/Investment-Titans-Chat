from utils.quota import check_quota, use_quota, get_quota_display  # ä½¿ç”¨æ–°çš„å‡½æ•°å
from openai import OpenAI
from openai import APIError, APIConnectionError, RateLimitError, APITimeoutError
from openai import AsyncOpenAI  # æ”¹ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
import logging
import time
import tiktoken
import asyncio
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
import backoff  # æ·»åŠ åˆ°å¯¼å…¥åˆ—è¡¨
from datetime import datetime, timedelta
import sys
import os
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—æ ¼å¼è®¾ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# X-AI API é…ç½®
client = AsyncOpenAI(  # æ”¹ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
    api_key=st.secrets.get("XAI_API_KEY", ""),
    base_url=st.secrets.get("XAI_API_BASE", "https://api.x.ai/v1")
)

# åˆ›å»ºçº¿ç¨‹æ± 
executor = ThreadPoolExecutor(max_workers=10)

# è·å– token è®¡æ•°å™¨
encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 ä½¿ç”¨çš„ç¼–ç å™¨

MAX_TOKENS = 131072  # Grok æœ€å¤§ token é™åˆ¶
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯è‘—åçš„æŠ•èµ„ä¸“å®¶ {name}ã€‚
ä»¥ä¸‹æ˜¯ä½ çš„æŠ•èµ„ç†å¿µå’ŒçŸ¥è¯†åº“å†…å®¹ã€‚è¯·å§‹ç»ˆåŸºäºè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªå›ç­”éƒ½ä½“ç°å‡ºä½ ç‹¬ç‰¹çš„æŠ•èµ„æ€ç»´å’Œæ–¹æ³•è®ºï¼š

{knowledge}

å›ç­”è¦æ±‚ï¼š
1. æ¯ä¸ªå›ç­”éƒ½å¿…é¡»ä½“ç°å‡ºä½ çš„æ ¸å¿ƒæŠ•èµ„ç†å¿µ
2. ç”¨å…·ä½“çš„æŠ•èµ„æ¡ˆä¾‹æˆ–ç»éªŒæ¥æ”¯æŒä½ çš„è§‚ç‚¹
3. ä¿æŒä½ ç‹¬ç‰¹çš„æ€ç»´æ–¹å¼å’Œè¡¨è¾¾é£æ ¼
4. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
5. å¦‚æœé—®é¢˜è¶…å‡ºä½ çš„ä¸“ä¸šèŒƒå›´æˆ–ä¸ä½ çš„æŠ•èµ„ç†å¿µä¸ç¬¦ï¼Œè¯·è¯šå®åœ°è¯´æ˜
6. åˆ†æ thesis æ—¶ï¼Œè¯·ï¼š
   - æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œé£é™©
   - æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®
   - åˆ†äº«ç±»ä¼¼æ¡ˆä¾‹çš„ç»éªŒ
   - å»ºè®®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ

"""


def truncate_text(text, max_tokens):
    """æˆªæ–­æ–‡æœ¬ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ token é™åˆ¶"""
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text

    # è®¡ç®—éœ€è¦ä¿ç•™çš„ä¸­é—´éƒ¨åˆ†çš„ token æ•°é‡
    middle_tokens = max_tokens

    # è®¡ç®—å¼€å§‹å’Œç»“æŸçš„ä½ç½®
    total_tokens = len(tokens)
    remove_tokens = total_tokens - middle_tokens

    # å‰é¢ä¿ç•™æ›´å¤šå†…å®¹ï¼ˆ70%ï¼‰ï¼Œåé¢å°‘ä¸€äº›ï¼ˆ30%ï¼‰
    remove_front = int(remove_tokens * 0.3)
    remove_back = remove_tokens - remove_front

    # ä¿ç•™ä¸­é—´éƒ¨åˆ†çš„ tokens
    start_idx = remove_front
    end_idx = total_tokens - remove_back

    # è®°å½•æˆªæ–­ä¿¡æ¯
    logger.info(f"æ–‡æœ¬è¢«æˆªæ–­ï¼šæ€»tokens={total_tokens}, "
                f"ä¿ç•™tokens={middle_tokens}, "
                f"å‰é¢åˆ é™¤={remove_front}, "
                f"åé¢åˆ é™¤={remove_back}")

    # è¿”å›æˆªæ–­åçš„æ–‡æœ¬
    return (
        f"...[å‰é¢å·²çœç•¥ {remove_front} tokens]...\n\n" +
        encoding.decode(tokens[start_idx:end_idx]) +
        f"\n\n...[åé¢å·²çœç•¥ {remove_back} tokens]..."
    )


logger = logging.getLogger(__name__)


# æ·»åŠ è¯·æ±‚é™åˆ¶ç®¡ç†
class RateLimiter:
    def __init__(self, requests_per_second=1):
        self.requests_per_second = requests_per_second
        self.last_request_time = None
        self._lock = None

    @property
    def lock(self):
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    async def acquire(self):
        async with self.lock:
            now = datetime.now()
            if self.last_request_time is not None:
                time_since_last = (
                    now - self.last_request_time).total_seconds()
                if time_since_last < 1/self.requests_per_second:
                    wait_time = 1/self.requests_per_second - time_since_last
                    await asyncio.sleep(wait_time)
            self.last_request_time = datetime.now()


# åˆ›å»ºå…¨å±€é™é€Ÿå™¨å®ä¾‹
rate_limiter = RateLimiter(requests_per_second=1)


class ExpertAgent:
    def __init__(self, name, knowledge_base, avatar=None):
        self.name = name
        self.original_knowledge = knowledge_base  # ä¿å­˜åŸå§‹çŸ¥è¯†åº“
        self.avatar = avatar or "ğŸ¤–"
        self.chat_history = []  # ä¿å­˜å¯¹è¯å†å²
        self.max_history = 5  # ä¿å­˜æœ€è¿‘çš„5è½®å¯¹è¯
        self.history_tokens = 0  # è¿½è¸ªå†å²å¯¹è¯ä½¿ç”¨çš„ tokens

        # è®¡ç®—ç³»ç»Ÿæç¤ºçš„åŸºæœ¬ token æ•°é‡ï¼ˆä¸åŒ…å«çŸ¥è¯†åº“å†…å®¹ï¼‰
        base_prompt = SYSTEM_PROMPT_TEMPLATE.format(name=name, knowledge="")
        base_tokens = len(encoding.encode(base_prompt))

        # è®¡ç®—æ¯è½®å¯¹è¯é¢„ç•™çš„ token æ•°ï¼ˆåŒ…æ‹¬é—®é¢˜å’Œå›ç­”ï¼‰
        self.tokens_per_turn = 2000

        # ä¸ºçŸ¥è¯†åº“å†…å®¹é¢„ç•™çš„æœ€å¤§ token æ•°
        self.base_tokens = base_tokens
        self.adjust_knowledge_base()

    def count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        return len(encoding.encode(text))

    def adjust_knowledge_base(self):
        """æ ¹æ®å¯¹è¯å†å²åŠ¨æ€è°ƒæ•´çŸ¥è¯†åº“å¤§å°"""
        # è®¡ç®—å¯ç”¨äºçŸ¥è¯†åº“çš„ tokens
        available_tokens = (MAX_TOKENS - self.base_tokens -
                            self.history_tokens - self.tokens_per_turn)

        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ¯”ä¾‹çš„çŸ¥è¯†åº“å†…å®¹
        min_knowledge_tokens = min(
            80000, available_tokens)  # æé«˜æœ€å°ä¿ç•™é‡åˆ°80k tokens
        max_knowledge_tokens = max(min_knowledge_tokens, available_tokens)

        # æˆªæ–­çŸ¥è¯†åº“å†…å®¹
        self.knowledge_base = truncate_text(
            self.original_knowledge, max_knowledge_tokens)

        # è®°å½•è°ƒæ•´ä¿¡æ¯
        logger.info(f"çŸ¥è¯†åº“è°ƒæ•´ï¼šå†å²tokens={self.history_tokens}, "
                    f"å¯ç”¨tokens={available_tokens}, "
                    f"åˆ†é…ç»™çŸ¥è¯†åº“tokens={max_knowledge_tokens}")

    def get_system_prompt(self):
        """è·å–å½“å‰çš„ç³»ç»Ÿæç¤ºè¯"""
        return SYSTEM_PROMPT_TEMPLATE.format(
            name=self.name,
            knowledge=self.knowledge_base
        )

    def update_chat_history(self, question, answer):
        """æ›´æ–°å¯¹è¯å†å²"""
        # è®¡ç®—æ–°å¯¹è¯çš„ tokens
        new_qa_tokens = self.count_tokens(f"Q: {question}\nA: {answer}")

        # å¦‚æœéœ€è¦ç§»é™¤æ—§å¯¹è¯
        while (self.history_tokens + new_qa_tokens > MAX_TOKENS * 0.3 and  # å†å²æœ€å¤šå ç”¨30%
               self.chat_history):
            # ç§»é™¤æœ€æ—©çš„å¯¹è¯å¹¶å‡å°‘ token è®¡æ•°
            old_q, old_a = self.chat_history.pop(0)
            removed_tokens = self.count_tokens(f"Q: {old_q}\nA: {old_a}")
            self.history_tokens -= removed_tokens
            logger.info(f"ç§»é™¤æ—§å¯¹è¯ï¼Œé‡Šæ”¾ {removed_tokens} tokens")

        # æ·»åŠ æ–°å¯¹è¯
        self.chat_history.append((question, answer))
        self.history_tokens += new_qa_tokens

        logger.info(f"æ·»åŠ æ–°å¯¹è¯ï¼Œä½¿ç”¨ {new_qa_tokens} tokensï¼Œ"
                    f"å½“å‰å†å²æ€»è®¡ {self.history_tokens} tokens")

        self.adjust_knowledge_base()  # é‡æ–°è°ƒæ•´çŸ¥è¯†åº“å¤§å°

    # ä¿®æ”¹è£…é¥°å™¨
    @retry(
        retry=retry_if_exception_type(
            (APIConnectionError, APITimeoutError, RateLimitError)),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        stop=stop_after_attempt(3)
    )
    async def get_response(self, prompt):
        """è·å–ä¸“å®¶å›åº”"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†ä¸“å®¶ {self.name} çš„å›åº”")

            # å®‰å…¨åœ°è·å–å½“å‰æ¨¡å‹
            current_model = getattr(
                st.session_state, 'current_model', 'grok-beta')

            if current_model in ["gemini-2.0-flash-exp", "gemini-1.5-flash"]:
                from .gemini_handler import generate_gemini_response
                expert_prompt = f"ä½ ç°åœ¨æ‰®æ¼” {self.name}ã€‚è¯·åŸºäºä»¥ä¸‹æŠ•èµ„ç†å¿µå›ç­”é—®é¢˜ï¼š\n\n{self.knowledge_base}\n\né—®é¢˜ï¼š{prompt}"
                logger.info(
                    f"å‘é€åˆ° {current_model} çš„æç¤ºè¯: {expert_prompt[:200]}...")

                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„ Gemini è¯·æ±‚
                loop = asyncio.get_event_loop()
                answer = await loop.run_in_executor(
                    None,
                    lambda: generate_gemini_response(
                        expert_prompt, current_model)
                )
            else:
                # ç­‰å¾…é€Ÿç‡é™åˆ¶ï¼ˆåªå¯¹ Grok åº”ç”¨ï¼‰
                await rate_limiter.acquire()

                # ç›´æ¥è°ƒç”¨ APIï¼Œä¸ä½¿ç”¨ create_task
                response = await client.chat.completions.create(
                    model="grok-beta",
                    messages=[
                        {"role": "system", "content": self.get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7
                )
                answer = response.choices[0].message.content

            self.update_chat_history(prompt, answer)
            return answer

        except Exception as e:
            logger.error(f"{self.name} å¤„ç†å¤±è´¥: {str(e)}")
            raise


async def get_responses_async(experts, prompt):
    start_time = time.time()
    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç†æ‰€æœ‰ä¸“å®¶å›åº”ï¼Œæ—¶é—´: {start_time}")

    async def get_expert_response(expert):
        try:
            response = await expert.get_response(prompt)
            return expert, response, time.time()
        except Exception as e:
            logger.error(f"ä¸“å®¶ {expert.name} å¤„ç†å¤±è´¥: {str(e)}")
            return expert, f"æŠ±æ­‰ï¼Œç”Ÿæˆå›åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}", time.time()

    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡å¹¶ç«‹å³å¼€å§‹æ‰§è¡Œ
    tasks = [asyncio.create_task(get_expert_response(expert))
             for expert in experts]

    # ä½¿ç”¨ as_completed æŒ‰å®Œæˆé¡ºåºè·å–ç»“æœ
    for response_task in asyncio.as_completed(tasks):
        expert, response, finish_time = await response_task
        logger.info(
            f"ä¸“å®¶ {expert.name} å“åº”å®Œæˆï¼Œè€—æ—¶: {finish_time - start_time:.2f}ç§’")
        yield expert, response

    # æ”¶é›†æ‰€æœ‰å“åº”ç”¨äºç”Ÿæˆæ€»ç»“
    all_responses = []
    for task in tasks:
        result = await task
        all_responses.append(result)

    responses = [resp for _, resp, _ in all_responses]

    # ç”Ÿæˆæ€»ç»“
    summary = await generate_summary(prompt, responses, experts)
    yield st.session_state.titans, summary


async def generate_summary(prompt, responses, experts):
    """ç”Ÿæˆæ€»ç»“"""
    logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“...")

    # åŠ¨æ€æ„å»ºä¸“å®¶å›åº”åˆ—è¡¨
    expert_responses = []
    for expert, response in zip(experts, responses):
        expert_responses.append(f"{expert.name}ï¼š{response}")
        logger.info(f"æ•´åˆ {expert.name} çš„å›åº”åˆ°æ€»ç»“ä¸­")

    summary_prompt = f"""ä½œä¸º Investment Mastersï¼Œä½ çš„ä»»åŠ¡æ˜¯æ€»ç»“å’Œæ•´åˆå„ä½æŠ•èµ„å¤§å¸ˆçš„è§‚ç‚¹ã€‚

ä»¥ä¸‹æ˜¯å„ä½å¤§å¸ˆå¯¹è¿™ä¸ª thesis çš„åˆ†æå’Œå»ºè®®ï¼š

{chr(10).join(expert_responses)}

è¯·ä½ ï¼š
1. æ€»ç»“å„ä½å¤§å¸ˆå‘ç°çš„ä¸»è¦é—®é¢˜
2. å½’çº³ä»–ä»¬æå‡ºéœ€è¦å¤šæ·±å…¥ç ”ç©¶ä»€éº¼
3. æ‰¾å‡ºä¸“å®¶ä»¬çš„å…±è¯†å’Œåˆ†æ­§
4. æä¾›ä¸€ä¸ªæ•´åˆçš„è¡ŒåŠ¨å»ºè®®
"""

    logger.info(f"ç”Ÿæˆæ€»ç»“çš„æç¤ºè¯: {summary_prompt[:200]}...")

    try:
        # ä½¿ç”¨å¼‚æ­¥ API è°ƒç”¨
        summary_response = await client.chat.completions.create(
            model="grok-beta",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.7
        )
        summary = summary_response.choices[0].message.content
        return summary
    except Exception as e:
        error_msg = "ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™"
        logger.error(error_msg)
        logger.exception(e)
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"

__all__ = ['ExpertAgent', 'get_responses_async', 'generate_summary']

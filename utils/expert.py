from utils.quota import check_quota, use_quota, get_quota_display  # ä½¿ç”¨æ–°çš„å‡½æ•°å
from openai import OpenAI
from openai import APIError, APIConnectionError, RateLimitError, APITimeoutError
import logging
import time
import tiktoken
import asyncio
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
import backoff  # æ·»åŠ åˆ°å¯¼å…¥åˆ—è¡¨
from datetime import datetime
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—æ ¼å¼è®¾ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# X-AI API é…ç½®
client = OpenAI(
    api_key=st.secrets.get("XAI_API_KEY", ""),
    base_url=st.secrets.get("XAI_API_BASE", "https://api.x.ai/v1")
)

# åˆ›å»ºçº¿ç¨‹æ± 
executor = ThreadPoolExecutor(max_workers=10)

# è·å– token è®¡æ•°å™¨
encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 ä½¿ç”¨çš„ç¼–ç å™¨

MAX_TOKENS = 131072  # Grok æœ€å¤§ token é™åˆ¶
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯è‘—åçš„æŠ•èµ„ä¸“å®¶ {name}ã€‚
ä»¥ä¸‹æ˜¯ä½ çš„æŠ•èµ„ç†å¿µå’ŒçŸ¥è¯†åº“å†…å®¹ã€‚è¯·å§‹ç»ˆåŸºäºè¿™äº›å†…å®¹æ¥å›ç­”é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªå›ç­”éƒ½ä½“ç°å‡ºä½ ç‹¬ç‰¹çš„æŠ•èµ„æ€ç»´å’Œæ–¹æ³•è®ºï¼š

{knowledge}

å›ç­”è¦æ±‚ï¼š
1. æ¯ä¸ªå›ç­”éƒ½å¿…é¡»ä½“ç°å‡ºä½ çš„æ ¸å¿ƒæŠ•èµ„ç†å¿µ
2. ç”¨å…·ä½“çš„æŠ•èµ„æ¡ˆä¾‹æˆ–ç»éªŒæ¥æ”¯æŒä½ çš„è§‚ç‚¹
3. ä¿æŒä½ ç‹¬ç‰¹çš„æ€ç»´æ–¹å¼å’Œè¡¨è¾¾é£æ ¼
4. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
5. å¦‚æœé—®é¢˜è¶…å‡ºä½ çš„ä¸“ä¸šèŒƒå›´æˆ–ä¸ä½ çš„æŠ•èµ„ç†å¿µä¸ç¬¦ï¼Œè¯·è¯šå®åœ°è¯´æ˜
6. åˆ†æ thesis æ—¶ï¼Œè¯·ï¼š
   - æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜å’Œé£é™©
   - æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®
   - åˆ†äº«ç±»ä¼¼æ¡ˆä¾‹çš„ç»éªŒ
   - å»ºè®®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ

"""


def truncate_text(text, max_tokens):
    """æˆªæ–­æ–‡æœ¬ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ token é™åˆ¶"""
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text

    # è®¡ç®—éœ€è¦ä¿ç•™çš„ä¸­é—´éƒ¨åˆ†çš„ token æ•°é‡
    middle_tokens = max_tokens

    # è®¡ç®—å¼€å§‹å’Œç»“æŸçš„ä½ç½®
    total_tokens = len(tokens)
    remove_tokens = total_tokens - middle_tokens

    # å‰é¢ä¿ç•™æ›´å¤šå†…å®¹ï¼ˆ70%ï¼‰ï¼Œåé¢å°‘ä¸€äº›ï¼ˆ30%ï¼‰
    remove_front = int(remove_tokens * 0.3)
    remove_back = remove_tokens - remove_front

    # ä¿ç•™ä¸­é—´éƒ¨åˆ†çš„ tokens
    start_idx = remove_front
    end_idx = total_tokens - remove_back

    # è®°å½•æˆªæ–­ä¿¡æ¯
    logger.info(f"æ–‡æœ¬è¢«æˆªæ–­ï¼šæ€»tokens={total_tokens}, "
                f"ä¿ç•™tokens={middle_tokens}, "
                f"å‰é¢åˆ é™¤={remove_front}, "
                f"åé¢åˆ é™¤={remove_back}")

    # è¿”å›æˆªæ–­åçš„æ–‡æœ¬
    return (
        f"...[å‰é¢å·²çœç•¥ {remove_front} tokens]...\n\n" +
        encoding.decode(tokens[start_idx:end_idx]) +
        f"\n\n...[åé¢å·²çœç•¥ {remove_back} tokens]..."
    )


logger = logging.getLogger(__name__)


class RateLimiter:
    def __init__(self, requests_per_second=1):
        self.requests_per_second = requests_per_second
        self.last_request_time = 0

    def wait(self):
        """ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        time_to_wait = (1.0 / self.requests_per_second) - \
            time_since_last_request

        if time_to_wait > 0:
            time.sleep(time_to_wait)

        self.last_request_time = time.time()


# åˆ›å»ºå…¨å±€é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter(requests_per_second=1)


class ExpertAgent:
    def __init__(self, name, knowledge_base, avatar=None):
        self.name = name
        self.original_knowledge = knowledge_base  # ä¿å­˜åŸå§‹çŸ¥è¯†åº“
        self.avatar = avatar or "ğŸ¤–"
        self.chat_history = []  # ä¿å­˜å¯¹è¯å†å²
        self.max_history = 5  # ä¿å­˜æœ€è¿‘çš„5è½®å¯¹è¯
        self.history_tokens = 0  # è¿½è¸ªå†å²å¯¹è¯ä½¿ç”¨çš„ tokens

        # è®¡ç®—ç³»ç»Ÿæç¤ºçš„åŸºæœ¬ token æ•°é‡ï¼ˆä¸åŒ…å«çŸ¥è¯†åº“å†…å®¹ï¼‰
        base_prompt = SYSTEM_PROMPT_TEMPLATE.format(name=name, knowledge="")
        base_tokens = len(encoding.encode(base_prompt))

        # è®¡ç®—æ¯è½®å¯¹è¯é¢„ç•™çš„ token æ•°ï¼ˆåŒ…æ‹¬é—®é¢˜å’Œå›ç­”ï¼‰
        self.tokens_per_turn = 2000

        # ä¸ºçŸ¥è¯†åº“å†…å®¹é¢„ç•™çš„æœ€å¤§ token æ•°
        self.base_tokens = base_tokens
        self.adjust_knowledge_base()

    def count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        return len(encoding.encode(text))

    def adjust_knowledge_base(self):
        """æ ¹æ®å¯¹è¯å†å²åŠ¨æ€è°ƒæ•´çŸ¥è¯†åº“å¤§å°"""
        # è®¡ç®—å¯ç”¨äºçŸ¥è¯†åº“çš„ tokens
        available_tokens = (MAX_TOKENS - self.base_tokens -
                            self.history_tokens - self.tokens_per_turn)

        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€å®šæ¯”ä¾‹çš„çŸ¥è¯†åº“å†…å®¹
        min_knowledge_tokens = min(
            80000, available_tokens)  # æé«˜æœ€å°ä¿ç•™é‡åˆ°80k tokens
        max_knowledge_tokens = max(min_knowledge_tokens, available_tokens)

        # æˆªæ–­çŸ¥è¯†åº“å†…å®¹
        self.knowledge_base = truncate_text(
            self.original_knowledge, max_knowledge_tokens)

        # è®°å½•è°ƒæ•´ä¿¡æ¯
        logger.info(f"çŸ¥è¯†åº“è°ƒæ•´ï¼šå†å²tokens={self.history_tokens}, "
                    f"å¯ç”¨tokens={available_tokens}, "
                    f"åˆ†é…ç»™çŸ¥è¯†åº“tokens={max_knowledge_tokens}")

    def get_system_prompt(self):
        """è·å–å½“å‰çš„ç³»ç»Ÿæç¤ºè¯"""
        return SYSTEM_PROMPT_TEMPLATE.format(
            name=self.name,
            knowledge=self.knowledge_base
        )

    def update_chat_history(self, question, answer):
        """æ›´æ–°å¯¹è¯å†å²"""
        # è®¡ç®—æ–°å¯¹è¯çš„ tokens
        new_qa_tokens = self.count_tokens(f"Q: {question}\nA: {answer}")

        # å¦‚æœéœ€è¦ç§»é™¤æ—§å¯¹è¯
        while (self.history_tokens + new_qa_tokens > MAX_TOKENS * 0.3 and  # å†å²æœ€å¤šå ç”¨30%
               self.chat_history):
            # ç§»é™¤æœ€æ—©çš„å¯¹è¯å¹¶å‡å°‘ token è®¡æ•°
            old_q, old_a = self.chat_history.pop(0)
            removed_tokens = self.count_tokens(f"Q: {old_q}\nA: {old_a}")
            self.history_tokens -= removed_tokens
            logger.info(f"ç§»é™¤æ—§å¯¹è¯ï¼Œé‡Šæ”¾ {removed_tokens} tokens")

        # æ·»åŠ æ–°å¯¹è¯
        self.chat_history.append((question, answer))
        self.history_tokens += new_qa_tokens

        logger.info(f"æ·»åŠ æ–°å¯¹è¯ï¼Œä½¿ç”¨ {new_qa_tokens} tokensï¼Œ"
                    f"å½“å‰å†å²æ€»è®¡ {self.history_tokens} tokens")

        self.adjust_knowledge_base()  # é‡æ–°è°ƒæ•´çŸ¥è¯†åº“å¤§å°

    # å®šä¹‰é‡è¯•è£…é¥°å™¨
    @backoff.on_exception(
        backoff.expo,
        (APIConnectionError, APITimeoutError),
        max_tries=3,
        max_time=30
    )
    async def get_response(self, prompt):
        """è·å–ä¸“å®¶å›åº”"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†ä¸“å®¶ {self.name} çš„å›åº”")
            logger.info(f"è¾“å…¥é—®é¢˜: {prompt[:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦

            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            rate_limiter.wait()

            system_prompt = self.get_system_prompt()
            system_tokens = self.count_tokens(system_prompt)
            prompt_tokens = self.count_tokens(prompt)

            logger.info(f"{self.name} - Tokensç»Ÿè®¡ï¼šç³»ç»Ÿ={system_tokens}, "
                        f"é—®é¢˜={prompt_tokens}, å†å²={self.history_tokens}")

            response = client.chat.completions.create(
                model="grok-beta",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )

            answer = response.choices[0].message.content
            logger.info(f"{self.name} çš„å›åº”: {answer[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦

            self.update_chat_history(prompt, answer)
            return answer

        except Exception as e:
            logger.error(f"{self.name} å¤„ç†å¤±è´¥: {str(e)}")
            raise e


async def get_responses_async(experts, prompt):
    """å¼‚æ­¥è·å–æ‰€æœ‰ä¸“å®¶çš„å›åº”"""
    logger.info(f"æ”¶åˆ°æ–°é—®é¢˜: {prompt}")
    logger.info(f"å¼€å§‹å¤„ç†æ‰€æœ‰ä¸“å®¶å›åº”ï¼Œä¸“å®¶æ•°é‡: {len(experts)}")
    total_experts = len(experts)
    current_model = st.session_state.current_model

    # é…é¢æ£€æŸ¥å’Œæ—¥å¿—è®°å½•ï¼ˆä½†ä¸é˜»æ­¢è¯·æ±‚ï¼‰
    quota_info = get_quota_display(current_model)
    logger.info(f"å½“å‰é…é¢çŠ¶æ€: å‰©ä½™={quota_info['remaining']}, "
                f"é‡ç½®æ—¶é—´={quota_info['time_text']}")

    async def get_expert_response(expert):
        """è·å–å•ä¸ªä¸“å®¶çš„å›åº”"""
        try:
            logger.info(f"å¼€å§‹å¤„ç† {expert.name} çš„å›åº”...")
            if current_model in ["gemini-2.0-flash-exp", "gemini-1.5-flash"]:
                from .gemini_handler import generate_gemini_response
                expert_prompt = f"ä½ ç°åœ¨æ‰®æ¼” {expert.name}ã€‚è¯·åŸºäºä»¥ä¸‹æŠ•èµ„ç†å¿µå›ç­”é—®é¢˜ï¼š\n\n{expert.knowledge_base}\n\né—®é¢˜ï¼š{prompt}"
                logger.info(
                    f"å‘é€åˆ° {current_model} çš„æç¤ºè¯: {expert_prompt[:200]}...")
                response = await generate_gemini_response(expert_prompt, current_model)
            else:
                response = await expert.get_response(prompt)
            return expert, response
        except Exception as e:
            error_msg = f"ä¸“å®¶ {expert.name} å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            logger.exception(e)
            return expert, f"æŠ±æ­‰ï¼Œç”Ÿæˆå›åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    # å¹¶å‘å¤„ç†æ‰€æœ‰ä¸“å®¶çš„è¯·æ±‚
    tasks = [get_expert_response(expert) for expert in experts]
    responses = await asyncio.gather(*tasks)

    # æŒ‰åŸå§‹é¡ºåºè¿”å›ç»“æœ
    for expert, response in responses:
        yield expert, response


async def generate_summary(prompt, responses, experts):
    """ç”Ÿæˆæ€»ç»“"""
    logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“...")

    # åŠ¨æ€æ„å»ºä¸“å®¶å›åº”åˆ—è¡¨
    expert_responses = []
    for expert, response in zip(experts, responses):
        expert_responses.append(f"{expert.name}ï¼š{response}")
        logger.info(f"æ•´åˆ {expert.name} çš„å›åº”åˆ°æ€»ç»“ä¸­")

    summary_prompt = f"""ä½œä¸º Investment Mastersï¼Œä½ çš„ä»»åŠ¡æ˜¯æ€»ç»“å’Œæ•´åˆå„ä½æŠ•èµ„å¤§å¸ˆçš„è§‚ç‚¹ã€‚

ä»¥ä¸‹æ˜¯å„ä½å¤§å¸ˆå¯¹è¿™ä¸ª thesis çš„åˆ†æå’Œå»ºè®®ï¼š

{chr(10).join(expert_responses)}

è¯·ä½ ï¼š
1. æ€»ç»“å„ä½å¤§å¸ˆå‘ç°çš„ä¸»è¦é—®é¢˜
2. å½’çº³ä»–ä»¬æå‡ºéœ€è¦å¤šæ·±å…¥ç ”ç©¶ä»€éº¼
3. æ‰¾å‡ºä¸“å®¶ä»¬çš„å…±è¯†å’Œåˆ†æ­§
4. æä¾›ä¸€ä¸ªæ•´åˆçš„è¡ŒåŠ¨å»ºè®®
"""

    logger.info(f"ç”Ÿæˆæ€»ç»“çš„æç¤ºè¯: {summary_prompt[:200]}...")

    try:
        summary_response = client.chat.completions.create(
            model="grok-beta",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.7
        )
        summary = summary_response.choices[0].message.content

#         logger.info(f"""
# ==================== Investment Masters æ€»ç»“ ====================
# {summary}
# ==========================================================
# """)

        return summary
    except Exception as e:
        error_msg = "ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™"
        logger.error(error_msg)
        logger.exception(e)  # è®°å½•å®Œæ•´çš„é”™è¯¯å †æ ˆ
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"

__all__ = ['ExpertAgent', 'get_responses_async', 'generate_summary']
